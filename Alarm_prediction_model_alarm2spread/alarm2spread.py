import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import networkx as nx
import random
import pickle
import os
from typing import List, Dict, Optional

from config import Config
from hierarchical_encoder import HierarchicalFusionEncoder
from policy_value_networks import PolicyNetwork, ValueNetwork, LocalRewardNetwork
from global_discriminator import GlobalDiscriminator
from replay_buffer import ReplayBuffer

class Alarm2Spread:
    def __init__(self, topology_graph, node_embeddings, real_paths, config=None):

        self.config = config or Config()
        self.topology_graph = topology_graph
        self.node_embeddings = node_embeddings
        self.real_paths = real_paths
        
        self.encoder = HierarchicalFusionEncoder(self.config)
        self.policy_net = PolicyNetwork(self.config)
        self.value_net = ValueNetwork(self.config)
        self.local_reward_net = LocalRewardNetwork(self.config)
        self.discriminator = GlobalDiscriminator(self.config)
        
        self.policy_optimizer = optim.Adam(
            list(self.policy_net.parameters()) + 
            list(self.encoder.parameters()),
            lr=self.config.LEARNING_RATE
        )
        self.value_optimizer = optim.Adam(
            self.value_net.parameters(), 
            lr=self.config.LEARNING_RATE
        )
        self.discriminator_optimizer = optim.Adam(
            self.discriminator.parameters(), 
            lr=self.config.LEARNING_RATE
        )
        self.reward_optimizer = optim.Adam(
            self.local_reward_net.parameters(),
            lr=self.config.LEARNING_RATE
        )
        
        self.memory = ReplayBuffer(self.config)

        self.step_count = 0
            """
        Args:
            topology_graph: Knowledge graph imported from the outside
            node_embeddings:  Node embeddings imported from the outside (generated by DeepWalkï¼‰
            real_paths: The real dissemination path imported from the outside
        """    
    def get_action_space(self, current_node: int, inference_path: List[int]) -> List[int]:
        if current_node not in self.topology_graph:
            return []
            
        neighbors = list(self.topology_graph.neighbors(current_node))
        valid_actions = [n for n in neighbors if n not in inference_path]
        return valid_actions
    
    def select_action(self, state: torch.Tensor, action_space_nodes: List[int]) -> int:

        if not action_space_nodes:
            return None
            

        action_embeddings = [self.node_embeddings[node] for node in action_space_nodes]
        

        with torch.no_grad():
            action_probs = self.policy_net(state, action_embeddings)
            if len(action_probs) > 0:
                action_idx = torch.multinomial(action_probs, 1).item()
                return action_space_nodes[action_idx]
        
        return None
    
    def compute_local_reward(self, state: torch.Tensor, action: torch.Tensor) -> float:

        with torch.no_grad():
            reward = self.local_reward_net.compute_reward(state, action)
            return reward.item()
    
    def predict_alarm_path(self, start_node: int) -> List[int]:

        if start_node not in self.topology_graph:
            return [start_node]
            
        current_node = start_node
        inference_path = [current_node]
        
        for step in range(self.config.MAX_STEPS):

            action_space = self.get_action_space(current_node, inference_path)
            if not action_space:
                break
                

            state = self.encoder(inference_path, self.topology_graph, self.node_embeddings)
            
            next_node = self.select_action(state, action_space)
            if next_node is None:
                break
                
            inference_path.append(next_node)
            current_node = next_node
        
        return inference_path
    
    def train_step(self):

        if len(self.memory) < self.config.BATCH_SIZE:
            return 0, 0, 0
            

        batch = self.memory.sample(self.config.BATCH_SIZE)
        if batch is None:
            return 0, 0, 0
            
        states, actions, rewards, next_states, dones, inference_paths = batch
        
        current_values = self.value_net(states)
        next_values = self.value_net(next_states)
        target_values = rewards + self.config.DISCOUNT_FACTOR * next_values * (~dones).float()
        
        value_loss = F.mse_loss(current_values.squeeze(), target_values.detach())
        
        td_errors = target_values - current_values
        
        action_probs_list = []
        for i, (state, action, inference_path) in enumerate(zip(states, actions, inference_paths)):


            action_probs = torch.tensor([0.5]) 
            action_probs_list.append(action_probs)
        
        action_probs = torch.cat(action_probs_list)
        policy_loss = -(torch.log(action_probs + 1e-8) * td_errors.detach()).mean()
        
   
        real_path_embeddings = self._paths_to_embeddings(self._sample_real_paths(self.config.BATCH_SIZE))
        generated_paths_embeddings = self._paths_to_embeddings(inference_paths)
        discriminator_loss = self.discriminator.compute_discriminator_loss(
            real_path_embeddings, generated_paths_embeddings
        )
        
        total_loss = policy_loss + value_loss + self.config.LAMBDA2 * discriminator_loss
        

        self.policy_optimizer.zero_grad()
        self.value_optimizer.zero_grad()
        self.discriminator_optimizer.zero_grad()
        
        total_loss.backward()
        
        self.policy_optimizer.step()
        self.value_optimizer.step()
        self.discriminator_optimizer.step()
        
        self.step_count += 1
        
        return policy_loss.item(), value_loss.item(), discriminator_loss.item()
    
    def _sample_real_paths(self, num_paths):
        if len(self.real_paths) <= num_paths:
            return self.real_paths
        return random.sample(self.real_paths, num_paths)
    
    def _paths_to_embeddings(self, paths):
        if not paths:
            return torch.zeros(1, self.config.EMBEDDING_DIM)
            
        batch_embeddings = []
        for path in paths:
            path_embedding = []
            for node in path:
                if node in self.node_embeddings:
                    path_embedding.append(self.node_embeddings[node])
                else:
                    path_embedding.append(torch.zeros(self.config.EMBEDDING_DIM))
            
            if path_embedding:
                path_tensor = torch.stack(path_embedding)
            else:
                path_tensor = torch.zeros(1, self.config.EMBEDDING_DIM)
            batch_embeddings.append(path_tensor)
        
        max_len = max(path_tensor.shape[0] for path_tensor in batch_embeddings)
        padded_embeddings = []
        for embedding in batch_embeddings:
            if embedding.shape[0] < max_len:
                padding = torch.zeros(max_len - embedding.shape[0], self.config.EMBEDDING_DIM)
                padded = torch.cat([embedding, padding], dim=0)
            else:
                padded = embedding
            padded_embeddings.append(padded)
        
        return torch.stack(padded_embeddings)
    
    def save_model(self, path):
        torch.save({
            'encoder_state_dict': self.encoder.state_dict(),
            'policy_net_state_dict': self.policy_net.state_dict(),
            'value_net_state_dict': self.value_net.state_dict(),
            'local_reward_net_state_dict': self.local_reward_net.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'policy_optimizer_state_dict': self.policy_optimizer.state_dict(),
            'value_optimizer_state_dict': self.value_optimizer.state_dict(),
            'discriminator_optimizer_state_dict': self.discriminator_optimizer.state_dict(),
        }, path)
    
    def load_model(self, path):
        checkpoint = torch.load(path)
        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])
        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.value_net.load_state_dict(checkpoint['value_net_state_dict'])
        self.local_reward_net.load_state_dict(checkpoint['local_reward_net_state_dict'])
        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
        self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state_dict'])
        self.value_optimizer.load_state_dict(checkpoint['value_optimizer_state_dict'])
        self.discriminator_optimizer.load_state_dict(checkpoint['discriminator_optimizer_state_dict'])